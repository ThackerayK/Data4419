{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5abf355-b61a-4c4a-8222-8f401ed24581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Step 1: Loads the basket data, applies efficient categorical types, \n",
    "    and builds mappings between product IDs and their internal matrix indices.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    df_baskets = pd.read_csv(file_path)\n",
    "    df_baskets['order_id'] = df_baskets['order_id'].astype('category')\n",
    "    df_baskets['product_id'] = df_baskets['product_id'].astype('category')\n",
    "    \n",
    "    product_ids = df_baskets['product_id'].cat.categories\n",
    "    product_to_index = {product: i for i, product in enumerate(product_ids)}\n",
    "    index_to_product = {i: product for product, i in product_to_index.items()}\n",
    "    \n",
    "    print(\"Data loaded and prepared successfully.\")\n",
    "    return df_baskets, product_to_index, index_to_product\n",
    "\n",
    "def build_item_order_matrix(df_baskets):\n",
    "    \"\"\"\n",
    "    Step 2: Builds a sparse item–order matrix (X) where X[i, j] = 1 \n",
    "    if product i appears in order j. \n",
    "    This representation enables efficient similarity computations.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 2: Building the item-order sparse matrix...\")\n",
    "    n_products = len(df_baskets['product_id'].cat.categories)\n",
    "    n_orders = len(df_baskets['order_id'].cat.categories)\n",
    "    \n",
    "    product_codes = df_baskets['product_id'].cat.codes.values\n",
    "    order_codes = df_baskets['order_id'].cat.codes.values\n",
    "    data = np.ones(len(df_baskets))\n",
    "    \n",
    "    item_order_matrix = csr_matrix((data, (product_codes, order_codes)), shape=(n_products, n_orders))\n",
    "    \n",
    "    print(\"Item-order matrix built successfully.\")\n",
    "    return item_order_matrix\n",
    "\n",
    "\n",
    "def compute_topk_neighbors(item_order_matrix, index_to_product, K=10, batch_size=500):\n",
    "    \"\"\"\n",
    "    Step 3 & 4: Efficiently computes Top-K similar items in batches using sparse \n",
    "    cosine similarity. Each product’s similarity scores are computed only for \n",
    "    relevant nonzero co-occurrences, reducing memory and runtime costs.\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 3 & 4: Computing Top-K neighbors (optimized batch method)...\")\n",
    "    start_time = time.time()\n",
    "    n_products = item_order_matrix.shape[0]\n",
    "    top_k_similar_items = {}\n",
    "\n",
    "    for start_idx in range(0, n_products, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_products)\n",
    "        batch = item_order_matrix[start_idx:end_idx]\n",
    "        \n",
    "        # Compute sparse cosine similarity only for this batch\n",
    "        sims = cosine_similarity(batch, item_order_matrix, dense_output=False)\n",
    "\n",
    "        for i, row in enumerate(sims):\n",
    "            global_i = start_idx + i\n",
    "            product_id = index_to_product[global_i]\n",
    "            row = row.toarray().ravel()\n",
    "\n",
    "            # Exclude self-similarity\n",
    "            row[global_i] = 0.0\n",
    "            \n",
    "            # Get top-K indices and scores\n",
    "            if np.any(row):\n",
    "                top_k_idx = np.argpartition(row, -K)[-K:]\n",
    "                top_k_scores = row[top_k_idx]\n",
    "                sort_order = np.argsort(top_k_scores)[::-1]\n",
    "                final_indices = top_k_idx[sort_order]\n",
    "                final_scores = top_k_scores[sort_order]\n",
    "                top_k_similar_items[product_id] = [\n",
    "                    {'product_id': index_to_product[idx], 'score': round(float(score), 4)}\n",
    "                    for idx, score in zip(final_indices, final_scores)\n",
    "                ]\n",
    "            else:\n",
    "                top_k_similar_items[product_id] = []\n",
    "\n",
    "        if (end_idx % (5 * batch_size)) == 0 or end_idx == n_products:\n",
    "            print(f\"Processed {end_idx}/{n_products} items...\")\n",
    "\n",
    "    print(f\"Top-K calculation done in {time.time() - start_time:.2f} seconds.\")\n",
    "    return top_k_similar_items\n",
    "\n",
    "\n",
    "def save_neighbors_to_file(top_k_items, file_path):\n",
    "    \"\"\"\n",
    "    Step 5: Saves the computed Top-K neighbor results to a CSV file for reuse. \n",
    "    Each row contains a product ID, its neighbor ID, and the similarity score.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving neighbors artifact to {file_path}...\")\n",
    "    rows = []\n",
    "    for product_id, neighbors in top_k_items.items():\n",
    "        for neighbor in neighbors:\n",
    "            rows.append([product_id, neighbor['product_id'], neighbor['score']])\n",
    "    \n",
    "    df_neighbors = pd.DataFrame(rows, columns=['product_id', 'neighbor_id', 'score'])\n",
    "    df_neighbors.to_csv(file_path, index=False)\n",
    "    print(\"Artifact saved successfully.\")\n",
    "\n",
    "def get_candidate_items(user_recent_purchases, top_k_similar_items, num_recommendations=5):\n",
    "    \"\"\"\n",
    "     Step 6: Generates candidate product recommendations for a user based on their \n",
    "    recent purchases. For each purchased item, it collects similar products \n",
    "    weighted by their similarity scores and returns the top-ranked candidates.\n",
    "    \"\"\"\n",
    "    candidate_items = defaultdict(float)\n",
    "    recent_set = set(user_recent_purchases)\n",
    "\n",
    "    for product_id in user_recent_purchases:\n",
    "        if product_id in top_k_similar_items:\n",
    "            for item in top_k_similar_items[product_id]:\n",
    "                candidate_product = item['product_id']\n",
    "                score = item['score']\n",
    "\n",
    "                if candidate_product in recent_set:\n",
    "                    continue\n",
    "                \n",
    "                if score > candidate_items[candidate_product]:\n",
    "                    candidate_items[candidate_product] = score\n",
    "\n",
    "    sorted_candidates = sorted(candidate_items.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [item[0] for item in sorted_candidates[:num_recommendations]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6182f2-3de9-4244-a77c-b49ae22c9f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading and preparing data...\n",
      "Data loaded and prepared successfully.\n",
      "\n",
      "Step 2: Building the item-order sparse matrix...\n",
      "Item-order matrix built successfully.\n",
      "\n",
      "Step 3 & 4: Computing Top-K neighbors (optimized batch method)...\n",
      "Processed 2500/49677 items...\n",
      "Processed 5000/49677 items...\n",
      "Processed 7500/49677 items...\n",
      "Processed 10000/49677 items...\n",
      "Processed 12500/49677 items...\n",
      "Processed 15000/49677 items...\n",
      "Processed 17500/49677 items...\n",
      "Processed 20000/49677 items...\n",
      "Processed 22500/49677 items...\n",
      "Processed 25000/49677 items...\n",
      "Processed 27500/49677 items...\n",
      "Processed 30000/49677 items...\n",
      "Processed 32500/49677 items...\n",
      "Processed 35000/49677 items...\n",
      "Processed 37500/49677 items...\n",
      "Processed 40000/49677 items...\n",
      "Processed 42500/49677 items...\n",
      "Processed 45000/49677 items...\n",
      "Processed 47500/49677 items...\n",
      "Processed 49677/49677 items...\n",
      "Top-K calculation done in 232.61 seconds.\n",
      "\n",
      "Saving neighbors artifact to top_k_neighbors.csv...\n",
      "Artifact saved successfully.\n",
      "\n",
      "Step 5: Generating candidate items for a user...\n",
      "\n",
      "Candidate items for a user who recently bought products [13176, 21137]:\n",
      "[47209, 27966, 21903, 24852, 39275]\n",
      "\n",
      "Top 3 Similar Items for a sample product (ID: 24852):\n",
      "[{'product_id': 47766, 'score': 0.1847}, {'product_id': 28204, 'score': 0.1649}, {'product_id': 21137, 'score': 0.1588}]\n",
      "\n",
      "Process finished successfully.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    DATA_FILE_PATH = r\"C:\\Users\\kthac\\Desktop\\Group_4419\\item item similarity\\item_item_basket.csv\"\n",
    "    NEIGHBORS_ARTIFACT_PATH = \"top_k_neighbors.csv\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df_baskets, product_to_index, index_to_product = load_and_prepare_data(DATA_FILE_PATH)\n",
    "    \n",
    "    if df_baskets is None:\n",
    "        return # Exit if data loading failed\n",
    "\n",
    "    # Build a sparse item–order matrix.\n",
    "    item_order_matrix = build_item_order_matrix(df_baskets)\n",
    "    \n",
    "    # Compute Top-K item similarities.\n",
    "    top_k_items = compute_topk_neighbors(item_order_matrix, index_to_product, K=10)\n",
    "    \n",
    "    # Save the results for future use\n",
    "    save_neighbors_to_file(top_k_items, NEIGHBORS_ARTIFACT_PATH)\n",
    "    \n",
    "    # Generate sample user recommendations.\n",
    "    print(\"\\nStep 5: Generating candidate items for a user...\")\n",
    "    \n",
    "    # Sample user purchases (using popular product IDs)\n",
    "    user_purchases = [13176, 21137]\n",
    "    recommended_candidates = get_candidate_items(user_purchases, top_k_items, num_recommendations=5)\n",
    "    \n",
    "    print(f\"\\nCandidate items for a user who recently bought products {user_purchases}:\")\n",
    "    print(recommended_candidates)\n",
    "    \n",
    "    # Show sample output for a specific product\n",
    "    print(\"\\nTop 3 Similar Items for a sample product (ID: 24852):\")\n",
    "    if 24852 in top_k_items and top_k_items[24852]:\n",
    "        top_3 = top_k_items[24852][:3]\n",
    "        print(top_3)\n",
    "    else:\n",
    "        print(\"Sample product ID 24852 not found or has no similar items.\")\n",
    "        \n",
    "    print(\"\\nProcess finished successfully.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babff94-429a-493c-bdc9-5625a2ba8a85",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Initial Preparation\n",
    "This step handles data ingestion and initial memory optimization using pandas.\n",
    "\n",
    "pd.read_csv() loads the basket data into a DataFrame.\n",
    "\n",
    ".astype('category') is the key memory-saving step. Instead of storing product and order IDs as large integers or strings, converting them to the category dtype lets pandas internally map each unique ID to a small integer (.cat.codes). This dramatically reduces memory usage and makes indexing more efficient.\n",
    "\n",
    "ID–Index Mapping: Two dictionaries (product_to_index and index_to_product) map original product_id values to their corresponding integer indices (0 to n_products − 1) and back. These mappings are essential for constructing the sparse matrix, which relies on zero-based numeric indices.\n",
    "\n",
    "# Step 2: Creating the Item-Order Sparse Matrix\n",
    "The goal is to represent which products appeared in which orders using a compact matrix form.\n",
    "\n",
    "A dense (num_products × num_orders) NumPy array would be too large, so we use a compressed sparse row (CSR) matrix:\n",
    "\n",
    "scipy.sparse.csr_matrix is chosen because it stores only nonzero entries and supports efficient matrix–vector and matrix–matrix operations, such as the dot product used later.\n",
    "\n",
    "Construction:\n",
    "csr_matrix((data, (rows, cols)), shape=...) builds the matrix from coordinate-style inputs:\n",
    "\n",
    "rows: product indices (df_baskets['product_id'].cat.codes)\n",
    "\n",
    "cols: order indices (df_baskets['order_id'].cat.codes)\n",
    "\n",
    "data: an array of ones indicating that a product appears in an order\n",
    "\n",
    "The result is a sparse binary matrix where X[i, j] = 1 if product i was part of order j.\n",
    "\n",
    "# Step 3: Calculate Item-Item Similarity \n",
    "This is the computational core of the algorithm. It measures how strongly products are associated based on how often they appear together.\n",
    "\n",
    "Co-occurrence Matrix:\n",
    "The dot product item_order_matrix.dot(item_order_matrix.T) computes a (num_products × num_products) co-occurrence matrix. Each cell counts how many orders contained both items.\n",
    "\n",
    "Memory Management:\n",
    "A dense similarity matrix would require tens of gigabytes of RAM, so the calculation is processed in chunks to remain memory-safe.\n",
    "\n",
    "Similarity Metric:\n",
    "Similarity is based on normalized co-occurrence: \n",
    "$$\\text{Similarity}(A, B) =\\frac{\\text{CoOccur}(A, B)}{\\sqrt{\\text{Count}(A)} \\times \\sqrt{\\text{Count}(B)}}$$\n",
    "\n",
    "The denominator is efficiently computed using NumPy’s outer product, avoiding explicit loops.\n",
    "\n",
    "Sparse Construction:\n",
    "Results are built incrementally into a LIL (List of Lists) sparse matrix, which allows fast row-by-row updates before being converted back to CSR format later.\n",
    "\n",
    "# Step 4: Finding Top-K Similar Items\n",
    "This step extracts the K most similar products for each item from the similarity matrix.\n",
    "\n",
    "The matrix is converted from LIL to CSR format (.tocsr()), which allows fast row access.\n",
    "\n",
    "Instead of using a full nearest-neighbor search (which would be slower and require a dense matrix), we use NumPy’s partial sorting:\n",
    "\n",
    "np.argsort(row)[::-1] retrieves indices in descending order of similarity.\n",
    "\n",
    "Slicing with [:K+1] selects the most similar K items (plus the item itself, which can later be excluded).\n",
    "\n",
    "This approach is both fast and memory-efficient, ideal for large-scale recommender systems.\n",
    "\n",
    "# Step 5: Generating Recommendations (Example Usage)\n",
    "Finally, the precomputed similarities are used to produce recommendations for a user.\n",
    "\n",
    "get_candidate_items() takes a user’s recent purchases and retrieves similar products from the Top-K lists.\n",
    "\n",
    "Candidate Aggregation:\n",
    "Each purchased item contributes a set of potential recommendations. A dictionary (candidate_items) accumulates all candidates and their highest similarity scores.\n",
    "\n",
    "Duplicate Handling:\n",
    "The check if candidate_product not in candidate_items or score > candidate_items[candidate_product] ensures that when a product is suggested by multiple items, only the strongest similarity score is kept.\n",
    "\n",
    "Ranking and Output:\n",
    "The dictionary is converted to a sorted list, returning the top-N recommended product IDs ranked by similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3f287-5c5f-45cd-ae93-3eb58fab49cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
